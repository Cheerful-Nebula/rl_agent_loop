import argparse
import ollama
from datetime import datetime 
import warnings
import os
warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")

# -- PROJECT IMPORTS --
import prompts  
from src.workspace_manager import ExperimentWorkspace
from src.code_validation import CodeValidator
from src import utils
from src.config import Config

MODEL_NAME = Config.LLM_MODEL

def run_agentic_improvement(iteration):
    # 1. Initialize Workspace
    ws = ExperimentWorkspace()
    print(f"üîµ AGENT (Iter {iteration}): Active in {ws.model_root_path}")
    
    # 2. Load Context (The "Eyes" of the Agent)
    # A. Metrics from the run just finished
    metrics = ws.load_metrics(iteration)
    if not metrics:
        print(f"‚ùå No metrics found for Iteration {iteration}. Cannot proceed.")
        return

    # B. The code that produced those metrics (Previous Iteration)
    if iteration == 1:
        prev_code_path = "seed_reward.py"
    else:
        prev_code_path = ws.get_path("code", iteration - 1, "reward.py")
    
    with open(prev_code_path, "r") as f:
        current_code = f.read()

    # C. Training Dynamics (Tensorboard Summary)
    tb_dir = ws.dirs["tensorboard"]
    training_summary = utils.summarize_training_log(str(tb_dir))

    # D. Long/Short Term Memory
    short_term_history = utils.get_recent_history(ws, iteration)
    long_term_memory = utils.get_long_term_memory(ws, iteration, MODEL_NAME)


    # =========================================================
    # PHASE 1: DIAGNOSIS (The "Brain")
    # =========================================================
    print("üîµ AGENT: Phase 1 - Diagnosing & Planning...")
    
    # Build Prompt using our new Clean Builder
    diag_role, diag_task = prompts.build_diagnosis_prompt(
        metrics=metrics,
        current_code=current_code,
        training_summary=training_summary,
        long_term_memory=long_term_memory,
        short_term_history=short_term_history
    )

    try:
        response = ollama.chat(model=MODEL_NAME, messages=[
            {'role': 'system', 'content': diag_role},
            {'role': 'user', 'content': diag_task}
        ])
        diagnosis_plan = response['message']['content']
        
        # Save the plan for human review
        plan_path = ws.get_path("cognition", iteration, "plan.md")
        with open(plan_path, "w") as f:
            f.write(diagnosis_plan)
        print(f"üìù Plan saved to {plan_path}")
            
    except Exception as e:
        print(f"‚ùå Phase 1 Error: {e}")
        return

    # =========================================================
    # PHASE 2: IMPLEMENTATION (The "Hands")
    # =========================================================
    print("üîµ AGENT: Phase 2 - Writing Code...")
    
    code_role, code_task = prompts.build_coding_prompt(diagnosis_plan, current_code)

    try:
        response2 = ollama.chat(model=MODEL_NAME, messages=[
            {'role': 'system', 'content': code_role},
            {'role': 'user', 'content': code_task}
        ])
        
        clean_code = utils.extract_python_code(response2['message']['content'])
        
        # --- VALIDATION LOOP ---
        validator = CodeValidator(clean_code)
        is_valid, feedback = validator.validate_static()
        if is_valid:
            is_valid, feedback = validator.validate_runtime()

        attempt_num = 0
        MAX_RETRIES = 5 # Reduced from 20 for speed
        
        while not is_valid and attempt_num < MAX_RETRIES:
            attempt_num += 1
            print(f"‚ö†Ô∏è Validation failed (Attempt {attempt_num}). Fixing...")
            
            fix_role, fix_task = prompts.build_fix_prompt(clean_code, feedback)
            
            response_fix = ollama.chat(model=MODEL_NAME, messages=[
                {'role': 'system', 'content': fix_role},
                {'role': 'user', 'content': fix_task}
            ])
            
            clean_code = utils.extract_python_code(response_fix['message']['content'])
            validator = CodeValidator(clean_code)
            is_valid, feedback = validator.validate_static()
            if is_valid:
                is_valid, feedback = validator.validate_runtime()

        # --- FINAL SAVE ---
        if is_valid:
            # Save to the CURRENT iteration's folder
            save_path = ws.get_path("code", iteration, "reward.py")
            
            timestamp_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            header = f"# Generated by {MODEL_NAME} (Iter {iteration}) on {timestamp_str}\n"
            
            with open(save_path, "w") as f:
                f.write(header + clean_code)
            print(f"‚úÖ Code validated and saved: {save_path}")
        else:
            print("‚ùå Failed to generate valid code.")

    except Exception as e:
        print(f"‚ùå Phase 2 Error: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--iteration", type=int, required=True)
    args = parser.parse_args()
    
    run_agentic_improvement(args.iteration)