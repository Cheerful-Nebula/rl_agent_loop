import argparse
import os
import json
import csv
import warnings
import time
from datetime import datetime, timedelta 
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import Logger, make_output_format
from pathlib import Path

# -- Custom IMPORTS --
from src.workspace_manager import ExperimentWorkspace
from src.utils import *
from src.callbacks import *
from src.evaluation import evaluate_agent
from src.config import Config 
from src.plot_progress import *

warnings.filterwarnings("ignore", message=".*pkg_resources is deprecated.*")

def run_training_cycle(iteration):
    start_time = time.perf_counter()
    # 1. Initialize Workspace, creates file structure for collected data
    ws = ExperimentWorkspace()
    print(f"üöÄ [Iter {iteration}] Initializing Training in: {ws.model_root_path}")

    # 2. Dynamic Code Loading
    # Logic: To train Iteration N, we need the code generated by Iteration N-1.
    # Load the code generated by the previous cycle
    reward_code_path = ws.get_path("code", iteration - 1, "reward.py")
    print(f"üì• Loading Reward Function from: {reward_code_path}")

    # 3. Setup Hardware, helper functions for hardware-aware hyperparameter optimization
    n_envs, device = get_hardware_config()
    ppo_params = get_optimized_ppo_params(n_envs, device)

    # 4. Create Environment, custom wrapper for injecting new Reward Function
    env = make_vec_env(lambda: make_env(reward_code_path), n_envs=n_envs, vec_env_cls=SubprocVecEnv)
    base_eval_env = make_env()
    shaped_eval_env = make_env(reward_code_path)

    # 5. Grab Paths for saving raw metrics and tensorboards
    logger_dir = ws.dirs["telemetry_training"] 
    tb_log_dir = str(ws.dirs["tensorboard"])
    suffix = f"_{iteration:02d}"

    # One subdirectory per iteration for TensorBoard
    tb_log_dir = Path(tb_log_dir) / f"Iter_{iteration:03d}"
    tb_log_dir.mkdir(parents=True, exist_ok=True)
    # Initialize Callbacks
    supervisor_callback = AgenticObservationTracker(obs_indices=[4, 6, 7], save_path=logger_dir)
    metrics_callback = ComprehensiveEvalCallback(threshold_score=200)
    entropy_callback = EntropyScheduleCallback(initial_ent_coef=0.1, final_ent_coef=0.00001, total_timesteps = Config.TOTAL_TIMESTEPS)
    progress_callback= FourWayEvalCallback(
        eval_env_base = base_eval_env,
        eval_env_shaped= shaped_eval_env,
        iteration = iteration,
        ws= ws,
        eval_freq = 100000,
        n_eval_episodes= 5,
        filename = "four_way_callback_eval.csv",
        verbose= 1,
    )
    output_formats = [
        # Optional: stdout logging
       # make_output_format("stdout", str(logger_dir), suffix),

        # Per-iteration CSV: telemetry_raw/progress_XXX.csv
        make_output_format("csv",        str(logger_dir), suffix),

        # Per-iteration TensorBoard events: tensorboard/Iter_XXX/events.out.tfevents...
        make_output_format("tensorboard", str(tb_log_dir), ""),
    ]
    logger = Logger(folder=str(logger_dir), output_formats=output_formats)

    lr_schedule = linear_schedule(1e-3, 3e-4)
    # 6. Train
    print(f"üèãÔ∏è Training on {device}...")
    model = PPO(
        "MlpPolicy",
        env,
        device=ppo_params['device'],
        n_steps=ppo_params['n_steps'],                   
        batch_size=ppo_params['batch_size'],
        learning_rate=lr_schedule,
        n_epochs=10,
        gamma=0.999,
        gae_lambda=0.98,
        ent_coef=0.03,
        verbose=0
    )
    print(f"Model training with n_steps: {ppo_params['n_steps']}, batch_size: {ppo_params['batch_size']}")
    model.set_logger(logger)
    # Use formatted string for TB log name
    model.learn(
        total_timesteps=Config.TOTAL_TIMESTEPS, 
        callback=[supervisor_callback, metrics_callback,progress_callback,entropy_callback]
    )
    
    # 7. Save & Evaluate
    model_save_path = ws.get_path("models", iteration, "model")
    model.save(model_save_path)
    
    print("üìä Running Evaluation...")

    # Collect Evaluation Statistics and Merge 
    base_det_stats, shaped_det_stats = evaluate_agent(model, 
                                                        iteration=iteration,
                                                        deterministic= True,
                                                        reward_code_path=reward_code_path, 
                                                        num_episodes=10)

    base_stoch_stats, shaped_stoch_stats = evaluate_agent(model, 
                                                             iteration=iteration,
                                                             deterministic= False,
                                                             reward_code_path=reward_code_path, 
                                                             num_episodes=10)

    stats_list = [base_det_stats, base_stoch_stats, shaped_det_stats, shaped_stoch_stats]
    # Create one CSV for final evaluation metrics, each iteration will add 4 rows:
    # Reward: Shaped vs. Base // Model Predictions: Deterministic vs. Stochastic
    csv_path = ws.dirs['telemetry'] / "final_eval.csv"
    file_exists = os.path.exists(csv_path)
    with open(csv_path, "a", newline="") as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(stats_list[0].keys())
        for stats in stats_list:
            writer.writerow(stats.values())

    for stats in stats_list:
        del stats['deterministic_flag']
        del stats['Iteration'] 
    # 7b. # Prepare Evaluation Data for handoff to the controller analyst
    training_dynamics = summarize_training_log(ws, iteration)

    
    # Creating plots of the training metrics for VLM
    plot_training_iteration(iteration,logger_dir/ f"progress_{iteration:02d}.csv")
    # 8. Data that will be given to controller analyst
    metrics_payload = {
        "timestamp": datetime.now().isoformat(),
        "iteration": iteration,
        "config": {"total_timesteps": Config.TOTAL_TIMESTEPS},
        "performance": stats_list,
        "training_dynamics": training_dynamics,
        "source_code_path": str(reward_code_path)
    }
    
    # Save the detailed JSON (Raw Data)
    ws.save_metrics(iteration, metrics_payload)
    end_time = time.perf_counter()
    elapsed_time = end_time - start_time
    # Format the output using timedelta
    print(f"‚úÖ Training Cycle Complete. Metrics passed to Controller.")
    print(f"Execution took: {timedelta(seconds=elapsed_time)}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--iteration", type=int, required=True)
    args = parser.parse_args()
    
    run_training_cycle(args.iteration)